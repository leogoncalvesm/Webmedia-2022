{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPVtWMMa5mFk"
      },
      "source": [
        "# Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLXz_wz-88ua",
        "outputId": "49e17729-7da6-4a0d-cf57-0bd8a3fcdae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "Collecting youtube-dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 30.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 46.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, ffmpeg\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=d013692e5ae80608cf40053402d506167b8731d5b340e3b4e3b93d751ecf25f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6084 sha256=c1a9bab94c43f3ffc1ea29f235f476a74e4c453d600dc1a9bf068bd3ac79ee4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/80/6e/caa3e16deb0267c3cbfd36862058a724144e19fdb9eb03af0f\n",
            "Successfully built sentence-transformers ffmpeg\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, youtube-dl, sentence-transformers, ffmpeg\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed ffmpeg-1.4 huggingface-hub-0.7.0 pyyaml-6.0 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.19.2 youtube-dl-2021.12.17\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn gdown sentence_transformers ffmpeg youtube-dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZU5OQpx5liX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from base64 import b64encode\n",
        "from pandas import DataFrame\n",
        "from datetime import timedelta\n",
        "from IPython.display import HTML\n",
        "from tensorflow import convert_to_tensor\n",
        "from tensorflow.keras import activations\n",
        "from sentence_transformers import CrossEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH5EUmal3TW8"
      },
      "source": [
        "# Summarization Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkoGZxWu3TEa"
      },
      "source": [
        "## Model downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz64M44Iw4t9",
        "outputId": "9de75897-c73f-4080-e4a8-63205cb7ecda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tZmDlrkV3MTNRiBasr7S-JQx_PTIeEdW\n",
            "To: /content/multilingual_subj_class_model.zip\n",
            "100% 661M/661M [00:08<00:00, 79.2MB/s]\n",
            "Archive:  multilingual_subj_class_model.zip\n",
            "   creating: multilingual_subj_class_model/\n",
            "  inflating: multilingual_subj_class_model/config.json  \n",
            "  inflating: multilingual_subj_class_model/pytorch_model.bin  \n",
            "  inflating: multilingual_subj_class_model/special_tokens_map.json  \n",
            "  inflating: multilingual_subj_class_model/tokenizer.json  \n",
            "  inflating: multilingual_subj_class_model/tokenizer_config.json  \n",
            "  inflating: multilingual_subj_class_model/vocab.txt  \n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1tZmDlrkV3MTNRiBasr7S-JQx_PTIeEdW\n",
        "!unzip multilingual_subj_class_model.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81uSh_WqJfb7"
      },
      "source": [
        "## Dataset downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROx8UVNA3Ym1",
        "outputId": "b15a64f0-6b12-44f0-c651-05abfbb44770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1epb3W1u8MDfHj9kBIlrR7mmcIX7QtVoj\n",
            "To: /content/cmu_mosi_subjectivity.pkl\n",
            "\r  0% 0.00/221k [00:00<?, ?B/s]\r100% 221k/221k [00:00<00:00, 95.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "## Downloading and reading CMU MUSI dataset\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1epb3W1u8MDfHj9kBIlrR7mmcIX7QtVoj\n",
        "\n",
        "df_data = pd.read_pickle('cmu_mosi_subjectivity.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRDaCm6CH_z7"
      },
      "source": [
        "## Summarization Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgFE5tctRouY"
      },
      "outputs": [],
      "source": [
        "## Filtering one video to summarize\n",
        "\n",
        "example_video_id = '9J25DZhivz8'\n",
        "\n",
        "# For a random video\n",
        "# example_video_id =  df_data.youtube_id.sample().iloc[0]\n",
        "\n",
        "video_data = df_data[df_data.youtube_id == example_video_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngibCC_s4sBJ",
        "outputId": "a9ff5c76-1784-48a8-bbe8-8122d09343ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "## Predicting subjectivity in video data\n",
        "\n",
        "# Loading model\n",
        "model_path = 'multilingual_subj_class_model'\n",
        "model = CrossEncoder(model_path, num_labels=2)\n",
        "\n",
        "# Data to predict\n",
        "num_rows = video_data.shape[0]\n",
        "sentences = video_data.transcript.to_numpy().reshape((num_rows, 1))\n",
        "\n",
        "# Predictions\n",
        "np_preds = activations.softmax(convert_to_tensor(model.predict(sentences))).numpy()\n",
        "\n",
        "# Predict classes (0 = Objective; 1 = Subjective)\n",
        "class_predictions = np.argmax(np_preds, axis=1)\n",
        "\n",
        "# Map label and assign to column\n",
        "video_data['subjectivity'] = np.where(class_predictions, 'subjective', 'objective')\n",
        "summ_video_data = video_data[video_data.subjectivity == 'subjective']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s-URQqh5kJb"
      },
      "outputs": [],
      "source": [
        "## Creating smaller version of the video\n",
        "\n",
        "# Download original video from youtube\n",
        "!youtube-dl https://youtube.com/watch?v={example_video_id} -o original_video.mp4\n",
        "\n",
        "# Cropping video into output\n",
        "def generate_ffmpeg_input(time_data: DataFrame,\n",
        "                          video_file_name:str='original_video.mp4') -> None:\n",
        "    \"\"\" This function generates a txt file with the\n",
        "    time windows to crop into the summarized video \"\"\"\n",
        "\n",
        "    next_start, next_end = 0, 0\n",
        "    subcjetive_segments = list(zip(time_data.time_start,\n",
        "                                   time_data.time_end))\n",
        "\n",
        "    segments = []\n",
        "    for start, end in subcjetive_segments:\n",
        "        if start <= next_end: next_end = end\n",
        "        else:\n",
        "            segments.append((next_start, next_end))\n",
        "            next_start, next_end = start, end\n",
        "    segments = segments[1:] + [(next_start, next_end)]\n",
        "\n",
        "    with open('input.txt', 'w') as f:\n",
        "        for start, end in segments:\n",
        "            f.write(f\"file '{video_file_name}'\\n\")\n",
        "            f.write(f\"inpoint {timedelta(seconds=start)}\\n\")\n",
        "            f.write(f\"outpoint {timedelta(seconds=end)}\\n\")\n",
        "\n",
        "time_frames = summ_video_data[['time_start', 'time_end']].sort_values('time_start')\n",
        "generate_ffmpeg_input(time_frames)\n",
        "\n",
        "!ffmpeg -safe 0 -f concat -segment_time_metadata 1 -i input.txt -vf select=concatdec_select -af aselect=concatdec_select,aresample=async=1 output.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDK2DtvT_AZ6"
      },
      "outputs": [],
      "source": [
        "## Playing videos\n",
        "\n",
        "original_file = 'original_video.mp4'\n",
        "output_file = 'output.mp4'\n",
        "\n",
        "orig_mp4 = open(original_file,'rb').read()\n",
        "out_mp4 = open(output_file,'rb').read()\n",
        "\n",
        "\n",
        "orig_data_url = \"data:video/mp4;base64,\" + b64encode(orig_mp4).decode()\n",
        "out_data_url = \"data:video/mp4;base64,\" + b64encode(out_mp4).decode()\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "    <h2>Original Video</h2>\n",
        "    <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    <br>\n",
        "    <h2>Summarized Video</h2>\n",
        "    <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % (orig_data_url, out_data_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eNNLhbzCwqY"
      },
      "source": [
        "# Datasets info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRYv3BLR5DU3"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-1TxvJ95Xl2"
      },
      "outputs": [],
      "source": [
        "# Create datasets directories\n",
        "!mkdir processed\\ datasets\n",
        "!mkdir raw\\ datasets/\n",
        "!mkdir raw\\ datasets/Subjectivity\\ Dataset\\ v1\n",
        "!mkdir raw\\ datasets/Book\\ Reviews\n",
        "!mkdir raw\\ datasets/Computer-BR\n",
        "!mkdir raw\\ datasets/HS-MVideoSumm\n",
        "!mkdir raw\\ datasets/Produtos\\ Eletronicos\n",
        "!mkdir raw\\ datasets/CMU-MOSI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzCkJ9vVP036",
        "outputId": "28e2bff3-b81a-4d4e-bb95-b7f98ec944c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-09 02:45:59--  http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 519599 (507K) [application/x-gzip]\n",
            "Saving to: ‘rotten_imdb.tar.gz’\n",
            "\n",
            "rotten_imdb.tar.gz  100%[===================>] 507.42K   539KB/s    in 0.9s    \n",
            "\n",
            "2022-06-09 02:46:01 (539 KB/s) - ‘rotten_imdb.tar.gz’ saved [519599/519599]\n",
            "\n",
            "--2022-06-09 02:46:01--  https://raw.githubusercontent.com/Lubelisa/Natural-Linguage-Processing/master/Corpus%20of%20Book%20Reviews/corpus_book_reviews_portuguese.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 55542 (54K) [text/plain]\n",
            "Saving to: ‘corpus_book_reviews_portuguese.csv’\n",
            "\n",
            "corpus_book_reviews 100%[===================>]  54.24K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-06-09 02:46:02 (3.87 MB/s) - ‘corpus_book_reviews_portuguese.csv’ saved [55542/55542]\n",
            "\n",
            "--2022-06-09 02:46:02--  https://github.com/Luizgferreira/subjectivity-classifier/raw/master/src/data/raw/Computer-BR.xlsx\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Luizgferreira/subjectivity-classifier/master/src/data/raw/Computer-BR.xlsx [following]\n",
            "--2022-06-09 02:46:02--  https://raw.githubusercontent.com/Luizgferreira/subjectivity-classifier/master/src/data/raw/Computer-BR.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 240446 (235K) [application/octet-stream]\n",
            "Saving to: ‘Computer-BR.xlsx’\n",
            "\n",
            "Computer-BR.xlsx    100%[===================>] 234.81K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-06-09 02:46:03 (7.54 MB/s) - ‘Computer-BR.xlsx’ saved [240446/240446]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QPPZpbUv381Rl0_cA1nC-hSqtRqyD5Ab\n",
            "To: /content/Ground truth - HSMVideoSumm.zip\n",
            "100% 26.5k/26.5k [00:00<00:00, 18.5MB/s]\n",
            "Archive:  Ground truth - HSMVideoSumm.zip\n",
            "   creating: raw datasets/HS-MVideoSumm/Bebê real/\n",
            "  inflating: raw datasets/HS-MVideoSumm/Bebê real/jornal_da_band.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Bebê real/jornal_da_record.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Bebê real/jornal_nacional.json  \n",
            "   creating: raw datasets/HS-MVideoSumm/Brumadinho/\n",
            "  inflating: raw datasets/HS-MVideoSumm/Brumadinho/jornal_da_band.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Brumadinho/jornal_da_record.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Brumadinho/jornal_nacional.json  \n",
            "   creating: raw datasets/HS-MVideoSumm/Encontro Bolsonaro e Trump/\n",
            "  inflating: raw datasets/HS-MVideoSumm/Encontro Bolsonaro e Trump/jornal_da_band.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Encontro Bolsonaro e Trump/jornal_da_record.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Encontro Bolsonaro e Trump/jornal_nacional.json  \n",
            "   creating: raw datasets/HS-MVideoSumm/Google e Huawei/\n",
            "  inflating: raw datasets/HS-MVideoSumm/Google e Huawei/jornal_da_band.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Google e Huawei/jornal_nacional.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Google e Huawei/sbt_brasil.json  \n",
            "   creating: raw datasets/HS-MVideoSumm/Incêndio Catedral Notre-Dame/\n",
            "  inflating: raw datasets/HS-MVideoSumm/Incêndio Catedral Notre-Dame/jornal_da_band.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Incêndio Catedral Notre-Dame/jornal_da_record.json  \n",
            "  inflating: raw datasets/HS-MVideoSumm/Incêndio Catedral Notre-Dame/jornal_nacional.json  \n",
            "--2022-06-09 02:46:05--  https://github.com/Luizgferreira/subjectivity-classifier/raw/master/src/data/raw/sentencas.xlsx\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Luizgferreira/subjectivity-classifier/master/src/data/raw/sentencas.xlsx [following]\n",
            "--2022-06-09 02:46:05--  https://raw.githubusercontent.com/Luizgferreira/subjectivity-classifier/master/src/data/raw/sentencas.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27386 (27K) [application/octet-stream]\n",
            "Saving to: ‘sentencas.xlsx’\n",
            "\n",
            "sentencas.xlsx      100%[===================>]  26.74K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-06-09 02:46:06 (10.4 MB/s) - ‘sentencas.xlsx’ saved [27386/27386]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1epb3W1u8MDfHj9kBIlrR7mmcIX7QtVoj\n",
            "To: /content/cmu_mosi_subjectivity.pkl\n",
            "100% 221k/221k [00:00<00:00, 62.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download Subjectivity Dataset v1\n",
        "!wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
        "!tar -xf rotten_imdb.tar.gz -C raw\\ datasets/Subjectivity\\ Dataset\\ v1\n",
        "\n",
        "# Download Book Reviews dataset\n",
        "!wget https://raw.githubusercontent.com/Lubelisa/Natural-Linguage-Processing/master/Corpus%20of%20Book%20Reviews/corpus_book_reviews_portuguese.csv\n",
        "!mv corpus_book_reviews_portuguese.csv raw\\ datasets/Book\\ Reviews/\n",
        "\n",
        "# Download Computer-BR dataset\n",
        "!wget https://github.com/Luizgferreira/subjectivity-classifier/raw/master/src/data/raw/Computer-BR.xlsx\n",
        "!mv Computer-BR.xlsx raw\\ datasets/Computer-BR/\n",
        "\n",
        "# Download HS-MVideoSumm\n",
        "!gdown https://drive.google.com/uc?id=1QPPZpbUv381Rl0_cA1nC-hSqtRqyD5Ab\n",
        "!unzip 'Ground truth - HSMVideoSumm.zip' -d raw\\ datasets/HS-MVideoSumm/ \n",
        "\n",
        "# Download Produtos Eletronicos\n",
        "!wget https://github.com/Luizgferreira/subjectivity-classifier/raw/master/src/data/raw/sentencas.xlsx\n",
        "!mv sentencas.xlsx raw\\ datasets/Produtos\\ Eletronicos/\n",
        "\n",
        "# Download CMU-MOSI\n",
        "!gdown https://drive.google.com/uc?id=1epb3W1u8MDfHj9kBIlrR7mmcIX7QtVoj\n",
        "!mv cmu_mosi_subjectivity.pkl raw\\ datasets/CMU-MOSI/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dbu0i7j5ihE"
      },
      "source": [
        "### HS-MVideoSumm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lFqUtm03AV7",
        "outputId": "089fffad-a8f2-457a-d1a6-5ddc26969d74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Neutro                            69\n",
              "Neutro - sem faces                37\n",
              "Entrevista                        23\n",
              "Opinião de repórter                7\n",
              "Opinião de repórter/entrevista     1\n",
              "Name: class, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Reading datasets into one dataframe\n",
        "\n",
        "hsmvideosumm_path = 'raw datasets/HS-MVideoSumm'\n",
        "datasets = []\n",
        "\n",
        "for root, subdirs, files in os.walk(hsmvideosumm_path):\n",
        "    for filename in files:\n",
        "        path = os.path.join(root, filename)\n",
        "        df = pd.read_json(path)\n",
        "        \n",
        "        df['path'], df['dataset'] = path, path.split('/')[1]\n",
        "        df['content_length'] = df.content.str.len()\n",
        "        df.drop(columns=['begin', 'end'], inplace=True)\n",
        "        \n",
        "        datasets.append(df)\n",
        "\n",
        "df = pd.concat(datasets)\n",
        "df['class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqGGc7GK6gTp",
        "outputId": "21e64b3d-c70c-47bd-bb4b-18d11e136e93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "objective     106\n",
              "subjective     31\n",
              "Name: subjectivity, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Mapping classes as either subjective or objective\n",
        "\n",
        "neutral_classes = ['Neutro', 'Neutro - sem faces']\n",
        "df['subjectivity'] = np.where(df['class'].isin(neutral_classes), 'objective', 'subjective')\n",
        "df.subjectivity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D95dR5Hz8aTf"
      },
      "outputs": [],
      "source": [
        "## Saving dataset\n",
        "\n",
        "save_path = 'processed datasets/hsmvideosumm.csv'\n",
        "df[['content', 'subjectivity']].to_csv(save_path,\n",
        "                                       sep=';',\n",
        "                                       encoding='utf-8',\n",
        "                                       index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_3yh2pu7NhL"
      },
      "source": [
        "### Computer-BR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARNp2BBs7N2y",
        "outputId": "2d89c526-f7fc-429b-9a00-7ebfdb29e0f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " 0    1677\n",
              "-1     407\n",
              " 1     197\n",
              "-2      36\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Reading original dataset Excel file\n",
        "\n",
        "df = pd.read_excel('raw datasets/Computer-BR/Computer-BR.xlsx')\n",
        "df = df[['Mensagem', 'FINAL']].rename(columns={'Mensagem': 'content',\n",
        "                                               'FINAL': 'polarity'})\n",
        "df.polarity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuRaLlsF7OUG",
        "outputId": "049a2f5f-ac7c-46bd-e681-2edfe9f4b466"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "objective     1677\n",
              "subjective     640\n",
              "Name: subjectivity, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Mapping polarity classes as either subjective or objective\n",
        "\n",
        "df['subjectivity'] = np.where(df.polarity == 0, 'objective', 'subjective')\n",
        "df.subjectivity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6tZLZ9M8XIk"
      },
      "outputs": [],
      "source": [
        "## Saving dataset\n",
        "\n",
        "save_path = 'processed datasets/computerbr.csv'\n",
        "df[['content', 'subjectivity']].to_csv(save_path,\n",
        "                                       sep=';',\n",
        "                                       encoding='utf-8',\n",
        "                                       index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilEHiQqx7PA_"
      },
      "source": [
        "### Book Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ASeejTz7PWG",
        "outputId": "a3a31b02-af36-43f8-abf6-70ca907a0bf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "objetiva     175\n",
              "subjetiva    175\n",
              "Name: OBJ/SUBJ, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Reading original dataset csv file\n",
        "\n",
        "df = pd.read_csv('raw datasets/Book Reviews/corpus_book_reviews_portuguese.csv')\n",
        "df = df[['FRASE', 'OBJ/SUBJ']].rename(columns={'FRASE': 'content'})\n",
        "df['OBJ/SUBJ'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koQLYcUR7P04",
        "outputId": "aeaf4712-092c-492b-936f-b605073c1588"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "objective     175\n",
              "subjective    175\n",
              "Name: subjectivity, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Renaming classes to English\n",
        "\n",
        "df['subjectivity'] = np.where(df['OBJ/SUBJ'] == 'objetiva', 'objective', 'subjective')\n",
        "df.subjectivity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8GK7lvp8bCV"
      },
      "outputs": [],
      "source": [
        "## Saving dataset\n",
        "\n",
        "save_path = 'processed datasets/bookreviews.csv'\n",
        "df[['content', 'subjectivity']].to_csv(save_path,\n",
        "                                       sep=';',\n",
        "                                       encoding='utf-8',\n",
        "                                       index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEiT49f7QF8"
      },
      "source": [
        "### Electronic Products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZRg9zRk7QcE",
        "outputId": "ea44a63e-8331-4694-bfc0-6746efeaf1ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " 1    131\n",
              "-1     59\n",
              " 0     43\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Reading original dataset Excel file\n",
        "\n",
        "df = pd.read_excel('raw datasets/Produtos Eletronicos/sentencas.xlsx')\n",
        "df = df[['Sentença', 'Polaridade']].rename(columns={'Sentença': 'content',\n",
        "                                                    'Polaridade': 'polarity'})\n",
        "df.polarity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlIGEYmG7RAT",
        "outputId": "62cee0c6-eed5-4f6a-c1f1-8ebb61e27cd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "subjective    190\n",
              "objective      43\n",
              "Name: subjectivity, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Mapping plarity classes as either subjective or objective\n",
        "\n",
        "df['subjectivity'] = np.where(df['polarity'] == 0, 'objective', 'subjective')\n",
        "df.subjectivity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kY7hz6f8cI-"
      },
      "outputs": [],
      "source": [
        "## Saving dataset\n",
        "\n",
        "save_path = 'processed datasets/electronicproducts.csv'\n",
        "df[['content', 'subjectivity']].to_csv(save_path,\n",
        "                                       sep=';',\n",
        "                                       encoding='utf-8',\n",
        "                                       index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smgnnSqgR4fo"
      },
      "source": [
        "### Subjectivity Dataset v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T3fHctySCLG",
        "outputId": "42926346-cfae-4de6-f557-85722fc35767"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "## Reading original dataset files\n",
        "\n",
        "dataset_path = 'raw datasets/Subjectivity Dataset v1'\n",
        "obj_set = os.path.join(dataset_path, 'plot.tok.gt9.5000')\n",
        "subj_set = os.path.join(dataset_path, 'quote.tok.gt9.5000')\n",
        "\n",
        "df_obj = pd.read_csv(obj_set, header=None, sep=';;;;', encoding=\"ISO-8859-1\")\n",
        "df_subj = pd.read_csv(subj_set, header=None, sep=';;;;', encoding=\"ISO-8859-1\")\n",
        "\n",
        "df_obj = df_obj.rename(columns={0: 'content'})\n",
        "df_subj = df_subj.rename(columns={0: 'content'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju5GATbwSCIW",
        "outputId": "0eb516d9-5b52-4d37-b8b1-a52f73c5066c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "objective     5000\n",
              "subjective    5000\n",
              "Name: subjectivity, dtype: int64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Concatenating subjective and objective sets with corresponding class column\n",
        "\n",
        "df_obj['subjectivity'] = 'objective'\n",
        "df_subj['subjectivity'] = 'subjective'\n",
        "\n",
        "df = df_obj.append(df_subj, ignore_index=True)\n",
        "df.subjectivity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryZOQWn5R5D9"
      },
      "outputs": [],
      "source": [
        "## Saving dataset\n",
        "\n",
        "save_path = 'processed datasets/subjectivitydatasetv1.csv'\n",
        "df[['content', 'subjectivity']].to_csv(save_path,\n",
        "                                       sep=';',\n",
        "                                       encoding='utf-8',\n",
        "                                       index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xjXSQS8Ijqs"
      },
      "source": [
        "### CMU-MOSI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kleQWHHQIkJx",
        "outputId": "1626ac72-b5a5-4cc2-cd9c-2da1bec4d8d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " 2.000000    120\n",
              "-1.000000    110\n",
              "-2.000000    103\n",
              " 0.000000     96\n",
              " 1.800000     94\n",
              " 2.200000     94\n",
              " 1.400000     92\n",
              "-1.600000     91\n",
              " 1.600000     90\n",
              "-1.400000     79\n",
              " 0.800000     77\n",
              "-1.800000     72\n",
              "-1.200000     71\n",
              "-0.400000     69\n",
              " 1.200000     68\n",
              "-0.800000     68\n",
              " 0.600000     63\n",
              " 0.400000     62\n",
              " 1.000000     62\n",
              " 0.200000     61\n",
              "-2.200000     61\n",
              "-0.600000     55\n",
              "-0.200000     55\n",
              " 2.400000     54\n",
              " 2.600000     44\n",
              "-2.400000     40\n",
              "-2.600000     36\n",
              "-2.800000     32\n",
              " 2.800000     29\n",
              "-0.500000     18\n",
              " 0.500000     17\n",
              " 3.000000     12\n",
              "-3.000000     12\n",
              " 0.250000     12\n",
              "-0.250000     10\n",
              "-1.500000     10\n",
              "-1.250000      8\n",
              "-0.750000      8\n",
              " 1.250000      7\n",
              "-1.750000      4\n",
              " 1.750000      4\n",
              " 0.750000      4\n",
              " 1.333333      4\n",
              "-0.666667      3\n",
              " 1.500000      3\n",
              "-0.333333      2\n",
              " 0.666667      2\n",
              "-2.250000      2\n",
              " 2.500000      2\n",
              "-1.333333      2\n",
              "-2.500000      2\n",
              " 2.250000      2\n",
              " 0.333333      1\n",
              "Name: subjectivity_score, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Reading original dataset\n",
        "\n",
        "dataset_path = 'raw datasets/CMU-MOSI/cmu_mosi_subjectivity.pkl'\n",
        "df = pd.read_pickle(dataset_path)\n",
        "df = df.rename(columns={'transcript': 'content'})\n",
        "df.subjectivity_score.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I-hQc9KOZem",
        "outputId": "09fbbfda-2c0f-4660-8832-9e890ee5d376"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "objective     1554\n",
              "subjective     645\n",
              "Name: subjectivity, dtype: int64"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Classifying subjectivity scores into either subjective or objective\n",
        "\n",
        "condition = (df.subjectivity_score <= -2) | (df.subjectivity_score >= 2) \n",
        "df['subjectivity'] = np.where(condition, 'subjective', 'objective')\n",
        "df.subjectivity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPbYt8DNPY3m"
      },
      "outputs": [],
      "source": [
        "## Saving dataset\n",
        "\n",
        "save_path = 'processed datasets/cmumosi.csv'\n",
        "df[['content', 'subjectivity']].to_csv(save_path,\n",
        "                                       sep=';',\n",
        "                                       encoding='utf-8',\n",
        "                                       index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxHePtpsC097"
      },
      "source": [
        "## Describing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfd5tc3SFaGP"
      },
      "outputs": [],
      "source": [
        "def token_data(content):\n",
        "    vect = CountVectorizer()\n",
        "    X = vect.fit_transform(content).toarray()\n",
        "\n",
        "    total_tokens = X.shape[1]\n",
        "    avg_tokens = np.array([i.sum() for i in X]).mean()\n",
        "\n",
        "    return total_tokens, np.round(avg_tokens, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnXn3hL8C2Vn"
      },
      "outputs": [],
      "source": [
        "datasets = ['bookreviews', 'computerbr', 'electronicproducts', 'hsmvideosumm', 'subjectivitydatasetv1', 'cmumosi']\n",
        "\n",
        "dfs = []\n",
        "tokens_data = {}\n",
        "\n",
        "for ds in datasets:\n",
        "    df = pd.read_csv(f'processed datasets/{ds}.csv', sep=';')\n",
        "    df['corpus'] = ds\n",
        "\n",
        "    total_tokens, avg_tokens = token_data(df.content)\n",
        "    tokens_data[ds] = {\n",
        "        'Avg. tokens by text': avg_tokens,\n",
        "        'Total tokens': total_tokens\n",
        "    }\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "tokens_data = pd.DataFrame(tokens_data).T\n",
        "df = pd.concat(dfs).reset_index(drop=True).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RWtUmsLFcck",
        "outputId": "4ea3986c-f1c4-4f5e-9f38-07d74968d62a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-31807017-e2ba-413d-a7f1-e8615d85d2fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>objective</th>\n",
              "      <th>subjective</th>\n",
              "      <th>Avg. tokens by text</th>\n",
              "      <th>Total tokens</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>corpus</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bookreviews</th>\n",
              "      <td>175</td>\n",
              "      <td>175</td>\n",
              "      <td>20.26</td>\n",
              "      <td>2472.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cmumosi</th>\n",
              "      <td>1554</td>\n",
              "      <td>645</td>\n",
              "      <td>11.75</td>\n",
              "      <td>3086.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>computerbr</th>\n",
              "      <td>1677</td>\n",
              "      <td>640</td>\n",
              "      <td>18.19</td>\n",
              "      <td>6323.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>electronicproducts</th>\n",
              "      <td>43</td>\n",
              "      <td>190</td>\n",
              "      <td>24.56</td>\n",
              "      <td>1779.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hsmvideosumm</th>\n",
              "      <td>106</td>\n",
              "      <td>31</td>\n",
              "      <td>43.14</td>\n",
              "      <td>1754.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subjectivitydatasetv1</th>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "      <td>20.71</td>\n",
              "      <td>20897.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31807017-e2ba-413d-a7f1-e8615d85d2fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31807017-e2ba-413d-a7f1-e8615d85d2fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31807017-e2ba-413d-a7f1-e8615d85d2fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       objective  subjective  Avg. tokens by text  \\\n",
              "corpus                                                              \n",
              "bookreviews                  175         175                20.26   \n",
              "cmumosi                     1554         645                11.75   \n",
              "computerbr                  1677         640                18.19   \n",
              "electronicproducts            43         190                24.56   \n",
              "hsmvideosumm                 106          31                43.14   \n",
              "subjectivitydatasetv1       5000        5000                20.71   \n",
              "\n",
              "                       Total tokens  \n",
              "corpus                               \n",
              "bookreviews                  2472.0  \n",
              "cmumosi                      3086.0  \n",
              "computerbr                   6323.0  \n",
              "electronicproducts           1779.0  \n",
              "hsmvideosumm                 1754.0  \n",
              "subjectivitydatasetv1       20897.0  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description = pd.pivot_table(df, index='corpus', values='index',\n",
        "                             columns='subjectivity', aggfunc='count')\n",
        "\n",
        "datasets_info = description.join(tokens_data)\n",
        "datasets_info"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FPVtWMMa5mFk",
        "wH5EUmal3TW8",
        "EkoGZxWu3TEa",
        "81uSh_WqJfb7",
        "cRDaCm6CH_z7",
        "4eNNLhbzCwqY",
        "iRYv3BLR5DU3",
        "6_3yh2pu7NhL",
        "ilEHiQqx7PA_",
        "fmEiT49f7QF8",
        "smgnnSqgR4fo",
        "8xjXSQS8Ijqs",
        "PxHePtpsC097"
      ],
      "name": "Video Summarization - Text Subjectivity Example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}